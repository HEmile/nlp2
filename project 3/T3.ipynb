{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1 - Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from statstracker import StatsTracker\n",
    "%pylab inline\n",
    "from utils import smart_reader, bitext_reader, iterate_minibatches, prepare_data\n",
    "from vocabulary import OrderedCounter, Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get vocab\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'\n",
    "\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)\n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)\n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get models for Task 3 \n",
    "from neuralibm1_T3_coll import NeuralIBM1Model_T3_coll\n",
    "\n",
    "# check neuralibm1trainer.py for the Trainer code\n",
    "from neuralibm1trainer import NeuralIBM1Trainer_T2_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run model task 3\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  print(\"Task 3\")\n",
    "  # some hyper-parameters\n",
    "  # tweak them as you wish\n",
    "  batch_size=128  # on CPU, use something much smaller e.g. 1-16\n",
    "  max_length=30\n",
    "  lr = 0.001\n",
    "  lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "  emb_dim = 64\n",
    "  mlp_dim = 128\n",
    "\n",
    "  stats_tracker = StatsTracker()\n",
    "  # our model\n",
    "  model = NeuralIBM1Model_T3_coll(\n",
    "    x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f,\n",
    "    batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "\n",
    "  # our trainer\n",
    "  trainer = NeuralIBM1Trainer_T2_gate(\n",
    "    model, train_e_path, train_f_path,\n",
    "    dev_e_path, dev_f_path, dev_wa,\n",
    "    num_epochs=10, batch_size=batch_size,\n",
    "    max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "  # now first TF needs to initialize all the variables\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # now we can start training!\n",
    "  print(\"Training started..\")\n",
    "  trainer.train(stats_tracker)\n",
    "    \n",
    "  print(\"Plotting the stats\")\n",
    "  stats_tracker.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
