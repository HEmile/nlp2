{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1 - Task 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from statstracker import StatsTracker\n",
    "from utils import smart_reader, bitext_reader, iterate_minibatches, prepare_data\n",
    "from vocabulary import OrderedCounter, Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get vocab\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'\n",
    "\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)\n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)\n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get models for Task 2 (concatenation & gate)\n",
    "from neuralibm1_T2_concat import NeuralIBM1Model_T2\n",
    "from neuralibm1_T2_gate import NeuralIBM1Model_T2_gate\n",
    "\n",
    "# check neuralibm1trainer.py for the Trainer code\n",
    "from neuralibm1trainer import NeuralIBM1Trainer_T2_gate, NeuralIBM1Trainer_T2_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 with concatenation\n",
      "Training with B=128 max_length=30 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss 55.083557 accuracy 0.18 lr 0.001000\n",
      "Iter   200 loss 60.977585 accuracy 0.19 lr 0.001000\n",
      "Iter   300 loss 57.624107 accuracy 0.22 lr 0.001000\n",
      "Iter   400 loss 51.516121 accuracy 0.21 lr 0.001000\n"
     ]
    }
   ],
   "source": [
    "# Run model task 2 with concatenation:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  print(\"Task 2 with concatenation\")\n",
    "  # some hyper-parameters\n",
    "  # tweak them as you wish\n",
    "  batch_size=128  # on CPU, use something much smaller e.g. 1-16\n",
    "  max_length=30\n",
    "  lr = 0.001\n",
    "  lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "  emb_dim = 64\n",
    "  mlp_dim = 128\n",
    "\n",
    "  stats_tracker = StatsTracker()\n",
    "  # our model\n",
    "  model = NeuralIBM1Model_T2(\n",
    "    x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f,\n",
    "    batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "\n",
    "  # our trainer\n",
    "  trainer = NeuralIBM1Trainer_T2_concat(\n",
    "    model, train_e_path, train_f_path,\n",
    "    dev_e_path, dev_f_path, dev_wa,\n",
    "    num_epochs=1, batch_size=batch_size,\n",
    "    max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "  # now first TF needs to initialize all the variables\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # now we can start training!\n",
    "  print(\"Training started..\")\n",
    "  trainer.train(stats_tracker)\n",
    "    \n",
    "  print(\"Plotting the stats\")\n",
    "  stats_tracker.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run model task 2 with gate:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  print(\"Task 2 with Gate\")\n",
    "  # some hyper-parameters\n",
    "  # tweak them as you wish\n",
    "  batch_size=128  # on CPU, use something much smaller e.g. 1-16\n",
    "  max_length=30\n",
    "  lr = 0.001\n",
    "  lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "  emb_dim = 64\n",
    "  mlp_dim = 128\n",
    "\n",
    "  stats_tracker = StatsTracker()\n",
    "  # our model\n",
    "  model = NeuralIBM1Model_T2_gate(\n",
    "    x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f,\n",
    "    batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "\n",
    "  # our trainer\n",
    "  trainer = NeuralIBM1Trainer_T2_gate(\n",
    "    model, train_e_path, train_f_path,\n",
    "    dev_e_path, dev_f_path, dev_wa,\n",
    "    num_epochs=10, batch_size=batch_size,\n",
    "    max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "  # now first TF needs to initialize all the variables\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # now we can start training!\n",
    "  print(\"Training started..\")\n",
    "  trainer.train(stats_tracker)\n",
    "    \n",
    "  print(\"Plotting the stats\")\n",
    "  stats_tracker.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
