{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1 - Task 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from statstracker import StatsTracker\n",
    "%pylab inline\n",
    "from utils import smart_reader, bitext_reader, iterate_minibatches, prepare_data\n",
    "from vocabulary import OrderedCounter, Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get vocab\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'\n",
    "test_e_path = 'data/test/test.e.gz'\n",
    "test_f_path = 'data/test/test.f.gz'\n",
    "test_wa = 'data/test/test.wa.nonullalign'\n",
    "\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)\n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "# pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)\n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "# pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))\n",
    "\n",
    "test_corpus = list(bitext_reader(\n",
    "            smart_reader(test_e_path),\n",
    "            smart_reader(test_f_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get models for Task 2 (concatenation & gate)\n",
    "from neuralibm1_T2_concat import NeuralIBM1Model_T2\n",
    "from neuralibm1_T2_gate import NeuralIBM1Model_T2_gate\n",
    "\n",
    "# check neuralibm1trainer.py for the Trainer code\n",
    "from neuralibm1trainer import NeuralIBM1Trainer_T2_gate, NeuralIBM1Trainer_T2_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model task 2 with concatenation:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  print(\"Task 2 with concatenation\")\n",
    "  # some hyper-parameters\n",
    "  # tweak them as you wish\n",
    "  batch_size=128  # on CPU, use something much smaller e.g. 1-16\n",
    "  max_length=30\n",
    "  lr = 0.001\n",
    "  lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "  emb_dim = 64\n",
    "  mlp_dim = 128\n",
    "\n",
    "  stats_tracker = StatsTracker()\n",
    "  # our model\n",
    "  model = NeuralIBM1Model_T2(\n",
    "    x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f,\n",
    "    batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "\n",
    "  # our trainer\n",
    "  trainer = NeuralIBM1Trainer_T2_concat(\n",
    "    model, train_e_path, train_f_path,\n",
    "    dev_e_path, dev_f_path, dev_wa,\n",
    "    num_epochs=5, batch_size=batch_size,\n",
    "    max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "  # now first TF needs to initialize all the variables\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # now we can start training!\n",
    "  print(\"Training started..\")\n",
    "  trainer.train(stats_tracker)\n",
    "    \n",
    "  test_aer, test_acc = trainer.model.evaluate(\n",
    "                test_corpus, test_wa)   \n",
    "  print(\"Testset accuracy: {}, AER: {}\".format(test_acc, test_aer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model task 2 with gate:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  print(\"Task 2 with Gate\")\n",
    "  # some hyper-parameters\n",
    "  # tweak them as you wish\n",
    "  batch_size=128  # on CPU, use something much smaller e.g. 1-16\n",
    "  max_length=30\n",
    "  lr = 0.001\n",
    "  lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "  emb_dim = 64\n",
    "  mlp_dim = 128\n",
    "\n",
    "  stats_tracker2 = StatsTracker()\n",
    "  # our model\n",
    "  model = NeuralIBM1Model_T2_gate(\n",
    "    x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f,\n",
    "    batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "\n",
    "  # our trainer\n",
    "  trainer = NeuralIBM1Trainer_T2_gate(\n",
    "    model, train_e_path, train_f_path,\n",
    "    dev_e_path, dev_f_path, dev_wa,\n",
    "    num_epochs=5, batch_size=batch_size,\n",
    "    max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "  # now first TF needs to initialize all the variables\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # now we can start training!\n",
    "  print(\"Training started..\")\n",
    "  trainer.train(stats_tracker2)\n",
    "\n",
    "  test_aer, test_acc = trainer.model.evaluate(\n",
    "                test_corpus, test_wa)   \n",
    "  print(\"Resulting AER: {}, accuracy: {}\".format(test_aer, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
